{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#gathering\">Gathering Data</a></li>\n",
    "<li><a href=\"#assessing\">Assessing Data</a></li>\n",
    "<li><a href=\"#cleaning\">Cleaning Data</a></li>\n",
    "<li><a href=\"#next\">Next Step</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "In this data wrangling project, I will first collect 3 different datasets that are related to WeRateDogs Twitter data. Then I will assess the data via both visual and programming assessment, and clean the data based on my assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset is `The WeRateDogs Twitter archive`, which I uploaded a csv file called 'twitter-archive-enhanced.csv'. I'll need its tweet IDs later.\n",
    "Then I used requests module to get tweet image predictions data using url 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'.\n",
    "The last dataset I gathered was additional data from Twitter API. To use Twitter API, I created my first Twitter account and applied for a developer account. Luckily, I passed the application! With a developer account, I created a project and got token and secret, which I used for authentication. Then I was able to use tweepy module to query Twitter's API for JSON data for each tweet ID in the `The WeRateDogs Twitter archive`. The data I extracted via API was tweet_id, favorite_count and retweet_count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first used visual assessment, both here and in Excel, and then programming assessment to find out the following data quality and tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "##### `Enhanced Twitter Archive` \n",
    "* Tweet_id column dtype is int, should be string.\n",
    "* Timestamp column dtype should be datetime.\n",
    "* There are 181 retweets in this dataset that need to be removed.\n",
    "* There are 78 replies in this dataset that need to be removed.\n",
    "* Not every tweet has identified dog stages.\n",
    "* Some of the rating denominators are not 10.\n",
    "* Wrong ratings like *'Meet Sam. She smiles 24/7 &amp; secretly aspires to be a reindeer.'*, where rating numerator & denominator are 24 & 7, and *'This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv'*, where rating numerator & denominator are 1 & 2.\n",
    "\n",
    "##### `image` Table\n",
    "* Tweet_id column dtype is int.\n",
    "* More than one predictions in for dogs in one single tweet.\n",
    "* There are non-dog predictions.\n",
    "\n",
    "##### `Data` Table\n",
    "* Count columns have object type instead of integer type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness\n",
    "##### `Enhanced Twitter Archive` \n",
    "* Dog stages are not in one column.\n",
    "* There are 2 columns related to rating, should be just one.\n",
    "\n",
    "##### `All three tables` \n",
    "* 3 datasets are not merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## Cleaning Data\n",
    "I defined the steps I would take to clean the datasets, wrote code to clean them, and finally, test out if my cleaning was complete.\n",
    "In the cleaning process, I made some functions to make cleaning more effient.\n",
    "After cleaning process, I merged the three datasets into a master dataset and export it as `twitter_archive_master.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next'></a>\n",
    "## Next Step\n",
    "This wraps up my wrangling process! Next I will analyze the clean (rather) `twitter_archive_master.csv` master dataset and find out some insights from the WeRateDogs Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook wrangle_report.ipynb to html\n",
      "[NbConvertApp] Writing 280866 bytes to wrangle_report.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html wrangle_report.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
